#Para sexo asumimos base intersexual
sexo_hombre = ifelse(X$sexo == 1, 1, 0)
sexo_mujer = ifelse(X$sexo == 2, 1, 0)
#Para actividad fisica asumimos base que no hace ejercicio
ejercicio_3veces = ifelse(X$actividad_fisica == 1, 1, 0)
ejercicio_2veces = ifelse(X$actividad_fisica == 2, 1, 0)
ejercicio_1veces = ifelse(X$actividad_fisica == 3, 1, 0)
#Para si ha fumado asumimos que base es NO
fumado_ocasional = ifelse(X$fumado_last30 == 1, 1, 0)
fumado_diario = ifelse(X$fumado_last30 == 2, 1, 0)
#Sacamos edad
edad = X$edad
X_nuevo = cbind(near_fabrica_si, near_basurero_si, near_buses_si, sexo_hombre, sexo_mujer, edad
, ejercicio_3veces, ejercicio_2veces, ejercicio_1veces, fumado_ocasional, fumado_diario)
fit=glm(Y~X_nuevo,family="binomial")
summary(fit)
#Es significativa la regresion
#Punto 6
library(ISLR)
data <- OJ
y = ifelse(data$Purchase == "CH", 1, 0)
data$Purchase
pCh <- data$PriceCH
pMM <- data$PriceMM
dCh <- data$DiscCH
dMM <- data$DiscMM
eCh <- data$SpecialCH
eMM <- data$SpecialMM
x <- cbind(rep(1,1070), pCh, pMM, dCh, dMM, eCh, eMM)
#1. Modelo
modelo <- glm(y ~ pCh + pMM + dCh + dMM + eCh + eMM, family = "binomial")
summary(modelo)
p_hat <- modelo$fitted.values
V <- diag(p_hat*(1-p_hat))
var_beta = solve(t(x)%*%V%*%x)
#2. Interpretar variables
exp(modelo$coefficients)
#3. Es el modelo significativo?
LR = (1430.9-1318.8)/(1069-1063)
cCritico <- qchisq(0.95,6)
LR
1-pchisq(LR, 6)
cCritico
#El modelo si es significativo
#4. Es cierto que las variables de si es especial o no, no son significativas
reducido <- glm(y ~ pCh + pMM + dCh + dMM, family = "binomial")
anova(reducido, modelo, test="LRT") #Si son significativas
#5. pCh = -pMM
c <- c(0, 1, 1, 0, 0, 0, 0)
theta <- t(modelo$coefficients)%*%c
var_theta <- t(c)%*%var_beta%*%c
ep1 <- theta^2/var_theta
pvalue <- 1 - pchisq(ep1,1)
pvalue
ep2 <- theta/sqrt(var_theta)
qnorm(1-.05/2)
#No rechazo, concluyo que la afirmacion es verdadera
#6. Intervalo para p si pCh = 1.5, pMM = 2.1, dCh = 0, dMM = 0.2, eCh = 0 eMM = 0
c <- c(1, 1.5, 2.1, 0, 0.2, 0, 0)
mu <- t(c)%*%modelo$coefficients
sup=mu+qnorm(0.975)*sqrt(t(c)%*%var_beta%*%c)
inf=mu-qnorm(0.975)*sqrt(t(c)%*%var_beta%*%c)
supp=exp(sup)/(1+exp(sup))
infpp=exp(inf)/(1+exp(inf))
interp=c(infpp,supp)*100
interp
library(readr)
datos_cardio_limpios <- read_csv("Documents/BI/Proyectos/ProyectoMedicina/DatosTemas/datos_cardio_limpios.csv")
#Prueba significancia estadistica
#Leemos los datos
datos = datos_cardio_limpios
#Sacamos X, Y
Y = ifelse(datos$enfermedad_cardio == 1, 1, 0)
X = datos_cardio_limpios[,-1]
head(X)
#Sacamos las variables dummies
#Para near fabrica asumimos base NO
near_fabrica_si = ifelse(X$near_fabrica == 1, 1, 0)
#Para near basurero asumimos base NO
near_basurero_si = ifelse(X$near_basurero == 1, 1, 0)
#Para near buses asumimos base NO
near_buses_si = ifelse(X$near_buses == 1, 1, 0)
#Para near servitecas asumimos base NO
near_servitecas_si = ifelse(X$near_servi == 1, 1, 0)
#Para sexo asumimos base intersexual
sexo_hombre = ifelse(X$sexo == 1, 1, 0)
sexo_mujer = ifelse(X$sexo == 2, 1, 0)
#Para actividad fisica asumimos base que no hace ejercicio
ejercicio_3veces = ifelse(X$actividad_fisica == 1, 1, 0)
ejercicio_2veces = ifelse(X$actividad_fisica == 2, 1, 0)
ejercicio_1veces = ifelse(X$actividad_fisica == 3, 1, 0)
#Para si ha fumado asumimos que base es NO
fumado_ocasional = ifelse(X$fumado_last30 == 1, 1, 0)
fumado_diario = ifelse(X$fumado_last30 == 2, 1, 0)
#Sacamos edad
edad = X$edad
X_nuevo = cbind(near_fabrica_si, near_basurero_si, near_buses_si, sexo_hombre, sexo_mujer, edad
, ejercicio_3veces, ejercicio_2veces, ejercicio_1veces, fumado_ocasional, fumado_diario)
fit=glm(Y~X_nuevo,family="binomial")
summary(fit)
#Es significativa la regresion?
LR = (nullDeviance-residualdeviance)/(null degrees-residual degrees)
#Es significativa la regresion?
LR = (394645-287267)/(541654-541643)
cCritico <- qchisq(0.95,541654-541643)
LR
1-pchisq(LR, 6)
cCritico
1-pchisq(LR, 541654-541643) #Si es significativo
library(readr)
datos_mental_limpios <- read_csv("Documents/BI/Proyectos/ProyectoMedicina/DatosTemas/datos_mental_limpios.csv")
#Prueba significancia estadistica datos Cardiovascular
#Leemos los datos
datos = datos_mental_limpios
head(datos)
Y = ifelse(datos$enfermedad_mental==1, 1, 0)
X = datos[,-1]
head(X)
#Sacamos los X y Y iniciales
Y = ifelse(datos$enfermedad_mental==1, 1, 0)
#Para sexo asumimos base intersexual
sexo_hombre = ifelse(datos$sexo==1, 1, 0)
sexo_mujer = ifelse(datos$sexo==2, 1, 0)
#Para tipo vivienda asumimos base CUARTO
vivienda_casa = ifelse(datos$tipo_vivienda == 1, 1, 0)
vivienda_apartamento = ifelse(datos$tipo_vivienda == 2, 1, 0)
humedad_no = ifelse(datos$humedad == 2, 1, 0)
#Para poca ventilacion asumimos base NO SABE
poca_ventilacion_si = ifelse(datos$mala_ventilacion == 1, 1, 0)
poca_ventilacion_no = ifelse(datos$mala_ventilacion == 2, 1, 0)
#Prueba significancia estadistica datos Cardiovascular
#Leemos los datos
datos = datos_mental_limpios
head(datos)
#Sacamos los X y Y iniciales
Y = ifelse(datos$enfermedad_mental==1, 1, 0)
X = datos[,-1]
head(X)
#Sacamos las variables dummies para las categorias
#Para sexo asumimos base intersexual
sexo_hombre = ifelse(datos$sexo==1, 1, 0)
sexo_mujer = ifelse(datos$sexo==2, 1, 0)
#Para tipo vivienda asumimos base CUARTO
vivienda_casa = ifelse(datos$tipo_vivienda == 1, 1, 0)
vivienda_apartamento = ifelse(datos$tipo_vivienda == 2, 1, 0)
#Para humedad asumimos base NO SABE
humedad_si = ifelse(datos$humedad == 1, 1, 0)
humedad_no = ifelse(datos$humedad == 2, 1, 0)
#Para poca ventilacion asumimos base NO SABE
poca_ventilacion_si = ifelse(datos$mala_ventilacion == 1, 1, 0)
poca_ventilacion_no = ifelse(datos$mala_ventilacion == 2, 1, 0)
#Creamos el X con las variables dummies
X_nuevo = cbind(edad, sexo_hombre)
#Variables numericas
edad = datos$edad
estrato = datos$estrato
numero_personas_casa = datos$num_personas_casa
head(X)
#Creamos el X con las variables dummies
X_nuevo = cbind(edad, sexo_hombre, sexo_mujer, vivienda_casa, vivienda_apartamento, numero_personas_casa,
humedad_si, humedad_no, poca_ventilacion_si, poca_ventilacion_no)
#Hacemos la regresion logistica
fit=glm(Y~X_nuevo,family="binomial")
summary(fit)
#Es significativa la regresion?
#LR = (nullDeviance-residualdeviance)/(null degrees-residual degrees)
#cCritico <- qchisq(0.95,diferencia null resigual)
LR = (91328-88154)/(611121-611111)
cCritico <- qchisq(0.95,611121-611111)
LR
1-pchisq(LR, 611121-611111) #Si es significativo
cCritico
#Punto 6
library(ISLR)
data <- OJ
y = ifelse(data$Purchase == "CH", 1, 0)
data$Purchase
pCh <- data$PriceCH
pMM <- data$PriceMM
dCh <- data$DiscCH
dMM <- data$DiscMM
eCh <- data$SpecialCH
eMM <- data$SpecialMM
x <- cbind(rep(1,1070), pCh, pMM, dCh, dMM, eCh, eMM)
#1. Modelo
modelo <- glm(y ~ pCh + pMM + dCh + dMM + eCh + eMM, family = "binomial")
summary(modelo)
p_hat <- modelo$fitted.values
V <- diag(p_hat*(1-p_hat))
var_beta = solve(t(x)%*%V%*%x)
#2. Interpretar variables
exp(modelo$coefficients)
#3. Es el modelo significativo?
LR = (1430.9-1318.8)/(1069-1063)
cCritico <- qchisq(0.95,6)
LR
1-pchisq(LR, 6)
cCritico
########################################################
#Solution
library(ISLR)
data=Default[1:500,]
fit=glm(default~.,data=data,family="binomial")
summary(fit)
LR=2920.6-1571.5
pvalor=1-pchisq(LR,3)
nul=glm(default~1,data=data,family="binomial")
anova(nul,fit,test="LRT")
nul2=glm(default~balance,data=data,family="binomial")
anova(nul2,fit,test="LRT")
student2=ifelse(data$student=="Yes",1,0)
data$student=student2
x=as.matrix(cbind(rep(1,dim(data)[1]),data[,2:4]))
var_beta=solve(t(x)%*%(diag(fit$fitted.values*(1-fit$fitted.values)))%*%x)
ep=(fit$coefficients[1]-2)/sqrt(var_beta[1,1])
########################################
c=c(0,0,1,-100)
theta=t(fit$coefficients)%*%c
var_theta=t(c)%*%var_beta%*%c
ep2=theta/sqrt(var_theta)
############################################
c=c(1,0,900,20000)
mu=t(c)%*%fit$coefficients
sup=mu+qnorm(0.975)*sqrt(t(c)%*%var_beta%*%c)
inf=mu-qnorm(0.975)*sqrt(t(c)%*%var_beta%*%c)
supp=exp(sup)/(1+exp(sup))
infpp=exp(inf)/(1+exp(inf))
interp=c(infpp,supp)
antropometric <- read.delim2("~/Documents/Tesis/Datos/anthropometric_data.meta")
View(antropometric)
otu_data <- read.delim("~/Documents/Tesis/Datos/otu_data.otus")
View(otu_data)
nutrients_data <- read.delim("~/Documents/Tesis/Datos/nutrients_data.txt")
View(nutrients_data)
bsp_classification <- read.delim("~/Documents/Tesis/Datos/bsp_classification.txt")
View(bsp_classification)
View(antropometric)
View(antropometric)
View(nutrients_data)
View(otu_data)
cags_data <- read.delim("~/Documents/Tesis/Datos/cags_data.cags")
View(cags_data)
View(antropometric)
View(nutrients_data)
View(bsp_classification)
phyla <- read.delim("~/Documents/Tesis/Datos/phyla.txt")
View(phyla)
taxonomiy_data <- read.delim("~/Documents/Tesis/Datos/taxonomiy_data.taxonomy")
View(taxonomiy_data)
otu_data <- read.delim("~/Documents/Tesis/Datos/otu_data.otus")
View(otu_data)
taxonomy_data <- read.delim("~/Documents/Tesis/Datos/taxonomy_data.taxonomy")
View(taxonomy_data)
phyla <- read.delim("~/Documents/Tesis/Datos/phyla.txt", header=FALSE)
View(phyla)
colSums(phyla)
colSums(phyla[,1])
colSums(phyla[1,])
phyla
library(readr)
phyla <- read_delim("Documents/Tesis/Datos/phyla.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
View(phyla)
colSums(phyla[])
colSums(phyla)
colSums(phyla[,1])
colSums(phyla[1,])
colSums(phyla[1,1])
phyla
# Libraries ----
library(GUniFrac) # To calculate UniFrac distances
install.packages("GUniFrac")
# Libraries ----
library(GUniFrac) # To calculate UniFrac distances
library(phytools) # To load phylogenetic tree
install.packages("phytools")
# Libraries ----
library(GUniFrac) # To calculate UniFrac distances
library(reshape2) # Function melt
library(phytools) # To load phylogenetic tree
library(reshape2) # Function melt
library(dplyr) # Function ddply
library(yarrr) # To make pirateplots
install.packages("yarrr")
library(yarrr) # To make pirateplots
library(rms) # To perform restricted cubic splines
install.packages("rms")
otu_data <- read.delim("~/Documents/Tesis/Datos/otu_data.otus")
View(otu_data)
microbio.rare = Rarefy(otu_data)$otu.tab.rff
otu_data
View(otu_data)
set.seed(5600)
microbio.rare = Rarefy(otu_data)$otu.tab.rff
otu_data[1,1]
otu_data[1,2]
microbio.rare = Rarefy(otu_data[,-c(2)])$otu.tab.rff
cosa = otu_data[,-c(2)]
View(cosa)
cosa = otu_data[-c(2),]
View(cosa)
cosa = otu_data[,-1]
View(cosa)
microbio.rare = Rarefy(cosa)$otu.tab.rff
View(microbio.rare)
sum(is.na(otu_data))
View(otu_data)
packages.install("pcalg")
install.packages("pcalg")
?rarefaction
?pcalg
library(pcalg)
install.packages("graph")
install.packages("graph")
library(pcalg)
install.packages("pcalg")
library(pcalg)
install.packages("graph")
if (!require("BiocManager", quietly = TRUE))
install.packages("BiocManager")
BiocManager::install("graph")
library(pcalg)
install.packages("RBGL")
BiocManager::install("RBGL")
library(pcalg)
?pcagls
?pcagl
-------------------------
### Initial commands ----
-------------------------
# Clean the workspace
#rm(list=ls())
# Set the seed to replicate analyses with random sampling
set.seed(12345)
# General function to copy-paste R tables into Excel
write.excel <- function(x,row.names=TRUE,col.names=TRUE,...) {
write.table(x,"clipboard",sep="\t",row.names=row.names,col.names=col.names,...)
}
# Color-blind-friendly palette
cbPalette = c("#E69F00","#56B4E9","#009E73","#F0E442","#0072B2","#D55E00","#CC79A7","#88CCEE","#CC6677","#DDCC77","#AA4499","#332288")
# Libraries
library(phytools)
library(GUniFrac)
library(data.table)
library(table1)
library(car)
library(reshape2)
library(NMF)
library(stringr)
library(randomForest)
# Create df_1 with specific row names
df_1 <- data.frame(x1 = c("A", "B", "C"), x2 = c(10, 20, 30))
rownames(df_1) <- c("r1", "r2", "r3")
# Create df_2 with matching and shuffled row names
df_2 <- data.frame(y1 = c("B", "C", "A", "D"), y2 = c(200, 300, 100, 400))
rownames(df_2) <- c("r2", "r3", "r1", "r4")  # Note: Order is different!
# Subset using %in% (order follows df_2)
Z1 <- df_2[rownames(df_2) %in% rownames(df_1), ]
print(Z1)  # Should retain the order from df_2: r2, r3, r1
#Packages
library(pcalg)
library(micd) #For mixed data tests
library(Rgraphviz) #Graph visualization
library(readr)
library(factoextra)
library(tibble)
library(dplyr)
#Directory
setwd("/Users/juanse/Documents/Tesis/IIND/HD_Microbiota_CI")
#Data Reading
{
#Anthropometric
anthro_data = read_delim("./Data/Consolidated/anthro_data.meta",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
#OTU observations
otu_data = read_delim("./Data/Consolidated/otu_data.otus",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
#Nutrient data
nutrients_data = read_delim("./Data/Consolidated/nutrients_data.txt",
delim = "\t", escape_double = FALSE,
trim_ws = TRUE)
unwanted_columns_nut = c("Codalt")
nutrients_data = nutrients_data[, !names(nutrients_data) %in% unwanted_columns_nut]
#Food group data
foodGroups_data = read.csv("./Data/Consolidated/food_groups_u24h.csv")
#Remove certain columns
unwanted_columns_fg = c("Code", "Codalt", "City", "r24h")
foodGroups_data = foodGroups_data[, !names(foodGroups_data) %in% unwanted_columns_fg]
}
#PCA preparation
{
#PCA Nutrients
nutrients_components = prcomp(nutrients_data[,-1], scale. = TRUE)
fviz_eig(nutrients_components)
#nutrient_values = nutrients_components$rotation
pca_nutrients <- as.data.frame(nutrients_components$x)
nutrients_pca_data = data.frame(ID = nutrients_data$ID,
PC_nutri_1 = pca_nutrients$PC1,
PC_nutri_2 =pca_nutrients$PC2,
PC_nutri_3 =pca_nutrients$PC3)
#PCA Food groups (sin HEI y SCORE GABAS)
fg_components = prcomp(foodGroups_data[,2:12], scale. = TRUE)
fviz_eig(fg_components)
#fg_rotations = fg_components$rotation
pca_fg = as.data.frame(fg_components$x)
fg_pca_data = data.frame(ID = foodGroups_data$ID,
PC_fg_1 = pca_fg$PC1,
PC_fg_2 = pca_fg$PC2,
PC_fg_3 = pca_fg$PC3)
}
#Numeric confounder histograms -> for checking normality
{
hist(anthro_data$age) #Not normal-looking
hist(anthro_data$bmi) #Gaussian-like
hist(anthro_data$body_fat) #Gaussian-like
hist(anthro_data$systolic_bp) #Gaussian-like
hist(anthro_data$diastolic_bp) #Gaussian-like
hist(anthro_data$triglycerides) #Not normal-looking
hist(anthro_data$HDL) #Gaussian-like
hist(anthro_data$glucose) #Not normal looking
hist(anthro_data$HOMA_IR) #Highly affected by outliers
anthro_data_no_out = anthro_data[anthro_data$HOMA_IR < 10,]
hist(anthro_data_no_out$HOMA_IR) #Gaussian-like (not really)
hist(anthro_data$hsCRP) #Highly affected by outliers
antrho_data_no_hscrp_out = anthro_data[anthro_data$hsCRP < 3,]
hist(antrho_data_no_hscrp_out$hsCRP) #Not normal-looking
hist(nutrients_pca_data$PC_nutri_1) #Gaussian-like
hist(nutrients_pca_data$PC_nutri_2) #Gaussian-like
hist(nutrients_pca_data$PC_nutri_3) #Seems affected by outliers
nutrients_pca_data_no_out = nutrients_pca_data[nutrients_pca_data$PC_nutri_3<5, ]
hist(nutrients_pca_data_no_out$PC_nutri_3) #Gaussian-like
hist(fg_pca_data$PC_fg_1) #Gaussian-like (barely, not so much)
hist(fg_pca_data$PC_fg_2) #Seems affected by otuliers
fg_pca_no_out = fg_pca_data[fg_pca_data$PC_fg_2 > (0-5),]
hist(fg_pca_no_out$PC_fg_2) #Gaussian-like
hist(fg_pca_data$PC_fg_3) #Gaussiasn-like
}
#Normalization of non-gaussian-looking confounders
{
anthro_data$age = scale(anthro_data$age)
anthro_data$triglycerides = scale(anthro_data$triglycerides)
anthro_data$glucose = scale(anthro_data$glucose)
anthro_data$hsCRP = scale(anthro_data$hsCRP)
}
#Creation of dummy variables
{
sex_yes = ifelse(anthro_data$sex=="Male", 1, 0)
city_BGA = ifelse(anthro_data$city=="Bucaramanga", 1, 0)
city_MED = ifelse(anthro_data$city=="Medellin", 1, 0)
city_BAR = ifelse(anthro_data$city=="Barranquilla", 1, 0)
city_CLI = ifelse(anthro_data$city=="Cali", 1, 0)
}
#Creation of confounding variables matrices: Anthro confounders, food-related confounders
{
anthro_confounders <- data.frame(
ID = anthro_data$ID,
sex_yes = sex_yes,
city_BAR = city_BAR,
city_MED = city_MED,
city_BGA = city_BGA,
city_CLI = city_CLI,
age = anthro_data$age,
bmi = anthro_data$bmi,
body_fat = anthro_data$body_fat,
systolic_bp = anthro_data$systolic_bp,
diastolic_bp = anthro_data$diastolic_bp,
triglycerides = anthro_data$triglycerides,
HDL = anthro_data$HDL,
glucose = anthro_data$glucose,
HOMA_IR = anthro_data$HOMA_IR,
hsCRP = anthro_data$hsCRP
)
food_related_confounders = inner_join(nutrients_pca_data, fg_pca_data, by = "ID")
}
#Identification of low observation OTUs
{
#OTU observation count
total_otu_observation_counts = colSums(otu_data[,-1])
#OTUs with at most 1 observation
zero_observation_otus = which(total_otu_observation_counts < 2)
length(zero_observation_otus) #There are 18 OTUs with no observations -> we dont need to use them
zero_observation_otus
#Add 1 for column indices (to account for ID)
zero_observation_otus = zero_observation_otus + 1
#Select only OTUs with observations
observed_otus= otu_data[,-zero_observation_otus]
#Sanity checks
observed_OTU_test = colSums(observed_otus[,-1])
length(which(observed_OTU_test == 0)) == 0
observed_OTU_test = colSums(observed_otus[,-1])
length(which(observed_OTU_test == 1)) == 0
}
#OTU transformations -> Relative abundances
{
#Relative abundance
otu_no_ids = observed_otus[,-1]
abundances = otu_no_ids / rowSums(otu_no_ids) * 100
otu_relative_abundances = cbind(observed_otus[,1], abundances)
}
#Creation of complete data matrix
{
#Join all data
complete_data_confounders = inner_join(anthro_confounders, food_related_confounders, by ="ID")
complete_data = inner_join(complete_data_confounders, otu_relative_abundances, by = "ID")
#Since we're working with Systolic BP, we remove MI_354_H because it has NA
complete_data = complete_data[!(complete_data$ID %in% c("MI_354_H")),]
#Remove ID Column
complete_data = as.data.frame(complete_data[,-1])
}
